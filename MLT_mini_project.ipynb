{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "Cx7t2NXHt7Mw",
    "outputId": "c13eb52f-b6a4-4e28-8bb8-e1d130349768"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       headline_text\n",
       "0  aba decides against community broadcasting lic...\n",
       "1     act fire witnesses must be aware of defamation\n",
       "2     a g calls for infrastructure protection summit\n",
       "3           air nz staff in aust strike for pay rise\n",
       "4      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"abcnews-date-text.csv\",error_bad_lines=False,usecols =[\"headline_text\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "2m4d0vv7q9hx",
    "outputId": "e4de443f-faaa-4291-84ba-ccffd94d9eee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1186018 entries, 0 to 1186017\n",
      "Data columns (total 1 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   headline_text  1186018 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 9.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# getting the info about the dataset\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "14QHq979uC5R"
   },
   "outputs": [],
   "source": [
    "# deleting duplicate headlines in dataset\n",
    "\n",
    "data = data.drop_duplicates('headline_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YaiiWtAruHAE"
   },
   "outputs": [],
   "source": [
    "# removing meaningless words and vectorizing the dataset\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "desc = data['headline_text'].values\n",
    "vectorizer = text.TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nigIdbxBuN2l",
    "outputId": "1eaf7829-0f6e-48bd-a25f-6163e9e0b626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101629\n",
      "['bands', 'bandstravaganza', 'bandt', 'bandts', 'banducci', 'banduk', 'bandung', 'bandwagon', 'bandwidth', 'bandy', 'bandyup', 'bane', 'banerjee', 'banerji', 'banesto', 'banfield', 'banfields', 'bang', 'banga', 'bangaldesh', 'bangaldeshi', 'bangalore', 'bangalow', 'bangarang', 'bangarra', 'bangarras', 'bangaru', 'bangas', 'bangay', 'banged', 'banger', 'bangers', 'banging', 'bangka', 'bangkok', 'bangkoks', 'bangla', 'bangladesh', 'bangladeshi', 'bangladeshis', 'bangladeshs', 'bangladsh', 'bangles', 'bango', 'bangor', 'bangs', 'bangtail', 'banh', 'banham', 'bani', 'banish', 'banished', 'banishes', 'banishing', 'banishment', 'banivanua', 'baniyala', 'baniyas', 'banjawarn', 'banjima', 'banjo', 'banjos', 'banjup', 'bank', 'banka', 'bankcard', 'bankcards', 'bankcruptcy', 'banked', 'banker', 'bankers', 'bankgok', 'bankholders', 'bankia', 'banking', 'banknote', 'banknotes', 'bankrobbers', 'bankroll', 'bankrolled', 'bankrolling', 'bankrolls', 'bankrupcy', 'bankrupt', 'bankruptcies', 'bankruptcuy', 'bankruptcy', 'bankrupticies', 'bankrupting', 'bankrupts', 'bankruptycy', 'banks', 'banksa', 'banksia', 'banksias', 'banksmeadow', 'banksters', 'bankstown', 'banksy', 'banksys']\n"
     ]
    }
   ],
   "source": [
    "# getting feature names i.e. keywords from vectored mappings\n",
    "\n",
    "word_features = vectorizer.get_feature_names()\n",
    "print(len(word_features))\n",
    "print(word_features[10000:10100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "POdp90AruSb4"
   },
   "outputs": [],
   "source": [
    "# creating tokenizer and stemmer\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ykPceddrub3b"
   },
   "outputs": [],
   "source": [
    "# defining the tokenization and stemming function\n",
    "\n",
    "def tokenize (text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7TWKusQug2t",
    "outputId": "3169a2fe-2bd6-4489-ebb1-f1902c703097"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imgun\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['timor', 'tip', 'titl', 'told', 'toll', 'toni', 'tough', 'tour', 'tourism', 'tourist', 'town', 'townsvill', 'track', 'trade', 'train', 'transport', 'travel', 'treatment', 'tree', 'tri', 'trial', 'tribut', 'troop', 'truck', 'trump', 'tsunami', 'turn', 'turnbul', 'tv', 'uk', 'uni', 'union', 'unit', 'univers', 'unveil', 'upgrad', 'urg', 'use', 'valley', 'veteran', 'vic', 'victim', 'victori', 'victoria', 'victorian', 'video', 'violenc', 'visit', 'volunt', 'vote', 'vow', 'vs', 'wa', 'wait', 'walk', 'wall', 'wallabi', 'want', 'war', 'warn', 'warrior', 'wast', 'watch', 'water', 'way', 'weather', 'week', 'weekend', 'welcom', 'welfar', 'west', 'western', 'whale', 'whi', 'white', 'wife', 'wild', 'william', 'win', 'wind', 'wine', 'winner', 'wit', 'withdraw', 'woe', 'woman', 'women', 'wont', 'work', 'worker', 'world', 'worri', 'year', 'yo', 'young', 'youth', 'zealand', 'zimbabw', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# concentrating the dataset by stemming the tokenized words in dataset\n",
    "\n",
    "vectorizer2 = text.TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X2 = vectorizer2.fit_transform(desc)\n",
    "word_features2 = vectorizer2.get_feature_names()\n",
    "print(word_features2[901:1000]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pBeSrtGstxFV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imgun\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:973: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  warnings.warn(\"'n_jobs' was deprecated in version 0.23 and will be\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=5, n_init=2, n_jobs=4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting data into kmeans\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters = 5, n_init = 2, n_jobs = 4) \n",
    "kmeans.fit(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kLP-lGEstr4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : polic, man, new, say, charg, australia, court, kill, govt, report, death, face, crash, nsw, attack, fund, murder, year, sydney, urg, water, wa, interview, jail, hit\n",
      "1 : council, plan, consid, rate, fund, urg, seek, new, merger, water, land, develop, reject, say, mayor, vote, elect, chang, rise, citi, meet, park, push, want, local\n",
      "2 : australian, warn, issu, open, polic, prompt, threat, south, share, year, dollar, spark, danger, new, market, risk, resid, farmer, health, flood, rise, weather, china, say, driver\n",
      "3 : plan, govt, new, water, say, group, develop, unveil, hous, chang, park, govern, labor, expans, public, resid, health, urg, centr, green, murray, opposit, reveal, shire, reject\n",
      "4 : win, award, cup, titl, open, gold, stage, world, final, tour, elect, australia, lead, seri, aussi, claim, second, australian, grand, england, big, race, record, m, battl\n"
     ]
    }
   ],
   "source": [
    "# We look at 5 the clusters generated by k-means.\n",
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-26:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(word_features2[word] for word in centroid))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "MLT_mini_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
